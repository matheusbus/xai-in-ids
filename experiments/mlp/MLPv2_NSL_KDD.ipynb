{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle  # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    Normalizer,\n",
    "    MaxAbsScaler,\n",
    "    RobustScaler,\n",
    "    PowerTransformer,\n",
    ")\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score  # for calculating accuracy of model\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")  # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    ")  # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense  # importing dense layer\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# representation of model layers\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Defining metric functions\n",
    "def ACC(TP, TN, FP, FN):\n",
    "    Acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "    return Acc\n",
    "\n",
    "\n",
    "def ACC_2(TP, FN):\n",
    "    ac = TP / (TP + FN)\n",
    "    return ac\n",
    "\n",
    "\n",
    "def PRECISION(TP, FP):\n",
    "    eps = 1e-7\n",
    "    Precision = TP / (TP + FP + eps)\n",
    "\n",
    "    return Precision\n",
    "\n",
    "\n",
    "def RECALL(TP, FN):\n",
    "    Recall = TP / (TP + FN)\n",
    "    return Recall\n",
    "\n",
    "\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "\n",
    "\n",
    "def BACC(TP, TN, FP, FN):\n",
    "    BACC = (TP / (TP + FN) + TN / (TN + FP)) * 0.5\n",
    "    return BACC\n",
    "\n",
    "\n",
    "def MCC(TP, TN, FP, FN):\n",
    "    eps = 1e-7\n",
    "    MCC = (TN * TP - FN * FP) / (\n",
    "        ((TP + FP + eps) * (TP + FN + eps) * (TN + FP + eps) * (TN + FN + eps)) ** 0.5\n",
    "    )\n",
    "    return MCC\n",
    "\n",
    "\n",
    "def AUC_ROC(y_test_bin, y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "        # print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "        auc_avg += auc(fpr[i], tpr[i])\n",
    "        counting = i + 1\n",
    "    return auc_avg / counting\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# attach the column names to the dataset\n",
    "feature = [\n",
    "    \"duration\",\n",
    "    \"protocol_type\",\n",
    "    \"service\",\n",
    "    \"flag\",\n",
    "    \"src_bytes\",\n",
    "    \"dst_bytes\",\n",
    "    \"land\",\n",
    "    \"wrong_fragment\",\n",
    "    \"urgent\",\n",
    "    \"hot\",\n",
    "    \"num_failed_logins\",\n",
    "    \"logged_in\",\n",
    "    \"num_compromised\",\n",
    "    \"root_shell\",\n",
    "    \"su_attempted\",\n",
    "    \"num_root\",\n",
    "    \"num_file_creations\",\n",
    "    \"num_shells\",\n",
    "    \"num_access_files\",\n",
    "    \"num_outbound_cmds\",\n",
    "    \"is_host_login\",\n",
    "    \"is_guest_login\",\n",
    "    \"count\",\n",
    "    \"srv_count\",\n",
    "    \"serror_rate\",\n",
    "    \"srv_serror_rate\",\n",
    "    \"rerror_rate\",\n",
    "    \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\n",
    "    \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\",\n",
    "    \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\",\n",
    "    \"label\",\n",
    "    \"difficulty\",\n",
    "]\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "\n",
    "train = \"KDDTrain+.txt\"\n",
    "test = \"KDDTest+.txt\"\n",
    "\n",
    "df = pd.read_csv(train, names=feature)\n",
    "df_test = pd.read_csv(test, names=feature)\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print(\"Dimensions of the Training set:\", df.shape)\n",
    "print(\"Dimensions of the Test set:\", df_test.shape)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df.drop([\"difficulty\"], axis=1, inplace=True)\n",
    "df_test.drop([\"difficulty\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "df[\"dst_host_same_srv_rate\"]\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "print(\"Label distribution Training set:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print()\n",
    "print(\"Label distribution Test set:\")\n",
    "print(df_test[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print(\"Training set:\")\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == \"object\":\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\n",
    "            \"Feature '{col_name}' has {unique_cat} categories\".format(\n",
    "                col_name=col_name, unique_cat=unique_cat\n",
    "            )\n",
    "        )\n",
    "\n",
    "# see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print(\"Distribution of categories in service:\")\n",
    "print(df[\"service\"].value_counts().sort_values(ascending=False).head())\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# Test set\n",
    "print(\"Test set:\")\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == \"object\":\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\n",
    "            \"Feature '{col_name}' has {unique_cat} categories\".format(\n",
    "                col_name=col_name, unique_cat=unique_cat\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "categorical_columns = [\"protocol_type\", \"service\", \"flag\"]\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns = [\"protocol_type\", \"service\", \"flag\"]\n",
    "# Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# protocol type\n",
    "unique_protocol = sorted(df.protocol_type.unique())\n",
    "string1 = \"Protocol_type_\"\n",
    "unique_protocol2 = [string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service = sorted(df.service.unique())\n",
    "string2 = \"service_\"\n",
    "unique_service2 = [string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag = sorted(df.flag.unique())\n",
    "string3 = \"flag_\"\n",
    "unique_flag2 = [string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols = unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "# do same for test set\n",
    "unique_service_test = sorted(df_test.service.unique())\n",
    "unique_service2_test = [string2 + x for x in unique_service_test]\n",
    "testdumcols = unique_protocol2 + unique_service2_test + unique_flag2\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "df_categorical_values_enc = df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc = testdf_categorical_values.apply(\n",
    "    LabelEncoder().fit_transform\n",
    ")\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(), columns=dumcols)\n",
    "# test set\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(\n",
    "    testdf_categorical_values_encenc.toarray(), columns=testdumcols\n",
    ")\n",
    "\n",
    "df_cat_data.head()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "trainservice = df[\"service\"].tolist()\n",
    "testservice = df_test[\"service\"].tolist()\n",
    "difference = list(set(trainservice) - set(testservice))\n",
    "string = \"service_\"\n",
    "difference = [string + x for x in difference]\n",
    "difference\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "newdf = df.join(df_cat_data)\n",
    "newdf.drop(\"flag\", axis=1, inplace=True)\n",
    "newdf.drop(\"protocol_type\", axis=1, inplace=True)\n",
    "newdf.drop(\"service\", axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test = df_test.join(testdf_cat_data)\n",
    "newdf_test.drop(\"flag\", axis=1, inplace=True)\n",
    "newdf_test.drop(\"protocol_type\", axis=1, inplace=True)\n",
    "newdf_test.drop(\"service\", axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# take label column\n",
    "labeldf = newdf[\"label\"]\n",
    "labeldf_test = newdf_test[\"label\"]\n",
    "# change the label column\n",
    "newlabeldf = labeldf.replace(\n",
    "    {\n",
    "        \"normal\": 0,\n",
    "        \"neptune\": 1,\n",
    "        \"back\": 1,\n",
    "        \"land\": 1,\n",
    "        \"pod\": 1,\n",
    "        \"smurf\": 1,\n",
    "        \"teardrop\": 1,\n",
    "        \"mailbomb\": 1,\n",
    "        \"apache2\": 1,\n",
    "        \"processtable\": 1,\n",
    "        \"udpstorm\": 1,\n",
    "        \"worm\": 1,\n",
    "        \"ipsweep\": 2,\n",
    "        \"nmap\": 2,\n",
    "        \"portsweep\": 2,\n",
    "        \"satan\": 2,\n",
    "        \"mscan\": 2,\n",
    "        \"saint\": 2,\n",
    "        \"ftp_write\": 3,\n",
    "        \"guess_passwd\": 3,\n",
    "        \"imap\": 3,\n",
    "        \"multihop\": 3,\n",
    "        \"phf\": 3,\n",
    "        \"spy\": 3,\n",
    "        \"warezclient\": 3,\n",
    "        \"warezmaster\": 3,\n",
    "        \"sendmail\": 3,\n",
    "        \"named\": 3,\n",
    "        \"snmpgetattack\": 3,\n",
    "        \"snmpguess\": 3,\n",
    "        \"xlock\": 3,\n",
    "        \"xsnoop\": 3,\n",
    "        \"httptunnel\": 3,\n",
    "        \"buffer_overflow\": 4,\n",
    "        \"loadmodule\": 4,\n",
    "        \"perl\": 4,\n",
    "        \"rootkit\": 4,\n",
    "        \"ps\": 4,\n",
    "        \"sqlattack\": 4,\n",
    "        \"xterm\": 4,\n",
    "    }\n",
    ")\n",
    "newlabeldf_test = labeldf_test.replace(\n",
    "    {\n",
    "        \"normal\": 0,\n",
    "        \"neptune\": 1,\n",
    "        \"back\": 1,\n",
    "        \"land\": 1,\n",
    "        \"pod\": 1,\n",
    "        \"smurf\": 1,\n",
    "        \"teardrop\": 1,\n",
    "        \"mailbomb\": 1,\n",
    "        \"apache2\": 1,\n",
    "        \"processtable\": 1,\n",
    "        \"udpstorm\": 1,\n",
    "        \"worm\": 1,\n",
    "        \"ipsweep\": 2,\n",
    "        \"nmap\": 2,\n",
    "        \"portsweep\": 2,\n",
    "        \"satan\": 2,\n",
    "        \"mscan\": 2,\n",
    "        \"saint\": 2,\n",
    "        \"ftp_write\": 3,\n",
    "        \"guess_passwd\": 3,\n",
    "        \"imap\": 3,\n",
    "        \"multihop\": 3,\n",
    "        \"phf\": 3,\n",
    "        \"spy\": 3,\n",
    "        \"warezclient\": 3,\n",
    "        \"warezmaster\": 3,\n",
    "        \"sendmail\": 3,\n",
    "        \"named\": 3,\n",
    "        \"snmpgetattack\": 3,\n",
    "        \"snmpguess\": 3,\n",
    "        \"xlock\": 3,\n",
    "        \"xsnoop\": 3,\n",
    "        \"httptunnel\": 3,\n",
    "        \"buffer_overflow\": 4,\n",
    "        \"loadmodule\": 4,\n",
    "        \"perl\": 4,\n",
    "        \"rootkit\": 4,\n",
    "        \"ps\": 4,\n",
    "        \"sqlattack\": 4,\n",
    "        \"xterm\": 4,\n",
    "    }\n",
    ")\n",
    "# put the new label column back\n",
    "newdf[\"label\"] = newlabeldf\n",
    "newdf_test[\"label\"] = newlabeldf_test\n",
    "print(newdf[\"label\"].head())\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "# Uncomment For top 20 features from SHAP Analysis\n",
    "# Specify your selected features. Note that you'll need to modify this list according to your final processed dataframe\n",
    "# selected_features = [\"dst_host_same_srv_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_count\",\"dst_host_serror_rate\",\n",
    "#                     \"root_shell\",\"dst_host_same_src_port_rate\",\"dst_host_count\",\"service_telnet\",\n",
    "#                     \"srv_rerror_rate\",\"hot\",\"same_srv_rate\",\"flag_REJ\",\"logged_in\",\"count\",\"srv_count\",\"service_http\",\"num_file_creations\",\"rerror_rate\",\"flag_SF\",\"flag_S0\",\"label\"]\n",
    "\n",
    "\n",
    "# Select those features from your dataframe\n",
    "# newdf = newdf[selected_features]\n",
    "# newdf_test = newdf_test[selected_features]\n",
    "\n",
    "# Now your dataframe only contains your selected features.\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)\n",
    "multi_data = newdf.copy()\n",
    "multi_label = pd.DataFrame(multi_data.label)\n",
    "\n",
    "multi_data_test = newdf_test.copy()\n",
    "multi_label_test = pd.DataFrame(multi_data_test.label)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# using standard scaler for normalizing\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "def standardization(df, col):\n",
    "    for i in col:\n",
    "        arr = df[i]\n",
    "        arr = np.array(arr)\n",
    "        df[i] = std_scaler.fit_transform(arr.reshape(len(arr), 1))\n",
    "    return df\n",
    "\n",
    "\n",
    "numeric_col = multi_data.select_dtypes(include=\"number\").columns\n",
    "data = standardization(multi_data, numeric_col)\n",
    "numeric_col_test = multi_data_test.select_dtypes(include=\"number\").columns\n",
    "data_test = standardization(multi_data_test, numeric_col_test)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2_test = preprocessing.LabelEncoder()\n",
    "enc_label = multi_label.apply(le2.fit_transform)\n",
    "enc_label_test = multi_label_test.apply(le2_test.fit_transform)\n",
    "multi_data = multi_data.copy()\n",
    "multi_data_test = multi_data_test.copy()\n",
    "\n",
    "multi_data[\"intrusion\"] = enc_label\n",
    "multi_data_test[\"intrusion\"] = enc_label_test\n",
    "\n",
    "# y_mul = multi_data['intrusion']\n",
    "multi_data\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "multi_data.drop(labels=[\"label\"], axis=1, inplace=True)\n",
    "multi_data\n",
    "multi_data_test.drop(labels=[\"label\"], axis=1, inplace=True)\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "y_train_multi = multi_data[[\"intrusion\"]]\n",
    "X_train_multi = multi_data.drop(labels=[\"intrusion\"], axis=1)\n",
    "\n",
    "print(\n",
    "    \"X_train has shape:\",\n",
    "    X_train_multi.shape,\n",
    "    \"\\ny_train has shape:\",\n",
    "    y_train_multi.shape,\n",
    ")\n",
    "\n",
    "y_test_multi = multi_data_test[[\"intrusion\"]]\n",
    "X_test_multi = multi_data_test.drop(labels=[\"intrusion\"], axis=1)\n",
    "\n",
    "print(\n",
    "    \"X_test has shape:\", X_test_multi.shape, \"\\ny_test has shape:\", y_test_multi.shape\n",
    ")\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_train_multi[\"intrusion\"])\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train_multi = LabelBinarizer().fit_transform(y_train_multi)\n",
    "y_train_multi\n",
    "\n",
    "y_test_multi = LabelBinarizer().fit_transform(y_test_multi)\n",
    "y_test_multi\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "Y_train = y_train_multi.copy()\n",
    "X_train = X_train_multi.copy()\n",
    "\n",
    "Y_test = y_test_multi.copy()\n",
    "X_test = X_test_multi.copy()\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Assuming you have features X and labels Y\n",
    "# X, Y = make_classification()\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy=\"minority\", random_state=100)\n",
    "\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "# Assume 'X_train' is your training data and 'X_test' your test data\n",
    "\n",
    "# Get feature names from the training set\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Reorder the test set to match the training set\n",
    "X_test = X_test[feature_names]\n",
    "\n",
    "# Wrap MLPClassifier with MultiOutputClassifier\n",
    "multi_target_mlp = MultiOutputClassifier(mlp)\n",
    "start = time.time()\n",
    "# Training the model\n",
    "multi_target_mlp.fit(X_train.values, Y_train)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print(f\"Time taken for training: {time_taken} seconds\")\n",
    "start = time.time()\n",
    "# Now you can predict the test set results\n",
    "y_pred = multi_target_mlp.predict(X_test)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print(f\"Time taken for pred: {time_taken} seconds\")\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "# Convert Y_test back to its original format\n",
    "y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "\n",
    "correctly_classified_indices = np.where(pred_labels == y_test)[0]\n",
    "misclassified_indices = np.where(pred_labels != y_test)[0]\n",
    "\n",
    "\n",
    "# In[89]:\n",
    "\n",
    "\n",
    "misclassified_indices[:5]\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "class_names = [\"Normal\", \"DoS\", \"Probe\", \"R2L\", \"U2R\"]\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# Create a Lime explainer object\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    training_labels=Y_train,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=class_names,\n",
    "    mode=\"classification\",\n",
    ")\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "# Select a correctly classified instance\n",
    "\n",
    "\n",
    "correct_instance = X_test.iloc[correctly_classified_indices[2]].values\n",
    "correct_exp = explainer.explain_instance(\n",
    "    correct_instance, multi_target_mlp.predict, num_features=41, top_labels=1\n",
    ")\n",
    "mr = np.random.randint(0, misclassified_indices.shape[0])\n",
    "misclassified_instance = X_test.iloc[misclassified_indices[0]].values\n",
    "\n",
    "\n",
    "# Explain this instance with LIME\n",
    "misclassified_exp = explainer.explain_instance(\n",
    "    misclassified_instance, multi_target_mlp.predict, num_features=122, top_labels=1\n",
    ")\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "misclassified_exp.as_list(label=0)  # 4\n",
    "\n",
    "\n",
    "# In[104]:\n",
    "\n",
    "\n",
    "print(y_test[misclassified_indices[0]], pred_labels[misclassified_indices[0]])\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "pred_labels = np.argmax(y_pred, axis=1)\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true_multiclass = np.argmax(Y_test, axis=1)\n",
    "confusion = confusion_matrix(y_true_multiclass, y_pred)\n",
    "\n",
    "# Binarize the output for AUC\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_true_multiclass)\n",
    "y_test_bin = lb.transform(y_true_multiclass)\n",
    "y_pred_bin = lb.transform(y_pred)\n",
    "\n",
    "# Iterate through each class and calculate the metrics\n",
    "class_names = [\"Normal\", \"DoS\", \"Probe\", \"R2L\", \"U2R\"]\n",
    "for i in range(len(class_names)):\n",
    "    TP = confusion[i, i]\n",
    "    FP = confusion[:, i].sum() - TP\n",
    "    FN = confusion[i, :].sum() - TP\n",
    "    TN = confusion.sum() - TP - FP - FN\n",
    "\n",
    "    # Call your metrics functions\n",
    "    Acc = ACC(TP, TN, FP, FN)\n",
    "    Precision = PRECISION(TP, FP)\n",
    "    Recall = RECALL(TP, FN)\n",
    "    F1_score = F1(Recall, Precision)\n",
    "    Balanced_accuracy = BACC(TP, TN, FP, FN)\n",
    "    Matthews = MCC(TP, TN, FP, FN)\n",
    "\n",
    "    # AUC_ROC calculation\n",
    "    AUC_ROC = roc_auc_score(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Metrics for: {class_names[i]}\")\n",
    "    print(\"Accuracy: \", Acc)\n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"F1: \", F1_score)\n",
    "    print(\"BACC: \", Balanced_accuracy)\n",
    "    print(\"MCC: \", Matthews)\n",
    "    print(\"AUC_ROC: \", AUC_ROC)\n",
    "    print()\n",
    "\n",
    "# AUC_ROC total\n",
    "print(\"AUC_ROC total: \", roc_auc_score(y_test_bin, y_pred_bin, multi_class=\"ovr\"))\n",
    "print(\n",
    "    \"---------------------------------------------------------------------------------\"\n",
    ")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "correctly_classified_indices = np.where(pred_labels == y_true_multiclass)[0]\n",
    "misclassified_indices = np.where(pred_labels != y_true_multiclass)[0]\n",
    "\n",
    "\n",
    "# Use KernelExplainer for model agnostic\n",
    "explainer = shap.KernelExplainer(multi_target_mlp.predict, shap.sample(X_train, 500))\n",
    "\n",
    "# Calculate Shap values on a small sample of test data\n",
    "small_X_test = X_test[:500]\n",
    "shap_values = explainer.shap_values(small_X_test)\n",
    "\n",
    "# Plot SHAP summary\n",
    "# shap.summary_plot(shap_values, small_X_test, feature_names=multi_data.columns)\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have a model \"multi_target_mlp\", and data \"X_train\" and \"X_test\"\n",
    "background_dataset = shap.sample(X_train, 500)\n",
    "explainer = shap.KernelExplainer(multi_target_mlp.predict, background_dataset)\n",
    "\n",
    "# Calculate Shap values on a small sample of test data\n",
    "small_X_test = X_test[:500]\n",
    "shap_values = explainer.shap_values(\n",
    "    small_X_test\n",
    ")  # shape is now [n_classes, n_samples, n_features]\n",
    "\n",
    "# Average absolute SHAP values across classes\n",
    "avg_shap_values = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(avg_shap_values, small_X_test, feature_names=multi_data.columns)\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "# Create a new matplotlib Figure and Axes\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# Plot SHAP summary on the created Axes\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    small_X_test,\n",
    "    feature_names=multi_data.columns,\n",
    "    class_names=[\"Normal\", \"DoS\", \"Probe\", \"R2L\", \"U2R\"],\n",
    ")\n",
    "# Save the figure\n",
    "plt.savefig(\"shap_summary_plot_MLP.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "# Create a new matplotlib Figure and Axes\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# Plot SHAP summary on the created Axes\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    small_X_test,\n",
    "    feature_names=multi_data.columns,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"shap_summary_plot.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
