Mudan√ßas necess√°rias:

Introdu√ß√£o:
 -OK- Revisar Objetivo Geral
 -OK- Revisar Objetivos Espec√≠ficos
 -OK- Revisar Justificativa
 -OK- Revisar Metodologia

Fundamenta√ß√£o:
 -OK- Escrevar t√≥picos sobre XAI
 -OK- Trabalhos relacionados

Desenvolvimento:

 -OK- In√≠cio do desenvolvimento deve ser retomado os objetivos e deixar bem claro as escolhas que eu fiz (revisar o problema, revisar a justificativa) - deixar bem claro a escolha dos modelos, escolha dos datasets

 -OK- Deixar claro qual foi a l√≥gica utilizada para converter o dataset dos tipos de ataque para bin√°rio (ataque ou benigno)

 -OK- Nos modelos, as afirma√ß√µes devem estar embasadas em fontes que d√£o suporte ao que eu afirmei

 -OK- Reavaliar se os pseudo-c√≥digos fazem sentido, deixar bem claro no texto apenas

 -OK- Sobre as figuras e tabelas, devem ser TODAS EXPLICADAS nos textos

 -OK- Reavaliar a utilidade das figuras 14 e 15 - analisar o que eu estava querendo dizer com elas se eu for manter

 -OK- Separar as matrizes de confus√£o em 3 imagens pois n√£o est√° leg√≠vel

 -OK MAS PODE MUDAR DE ACORDO COM ALTERA√á√ïES NO C√ìDIGO- Detalhar como foi implementado o XAI

Resultados:

- Reavaliar as dimens√µes dos gr√°ficos utilizados pois est√£o em escalas diferentes

- Fazer uma rela√ß√£o entre o SHAP e as m√©tricas de avalia√ß√£o da cada modelo, pois mesmo 1 modelo atingindo 100% de acur√°cia e outro atingindo um n√∫mero menor, eles parecem convergir para um mesmo lugar nos resultados do SHAP

- Fazer uma an√°lise/compara√ß√£o estat√≠stica dos resultados do SHAP

- Importante trazer uma discuss√£o mais expandida do que cada t√©cnica XAI deu como resultado


TODO:

- Extrair m√©tricas SHAP de cada um dos modelos
- Extrair m√©tricas LIME de cada um dos modelos

M√©tricas:
    
    A. Import√¢ncia dos Atributos (Global)
    B. Consist√™ncia entre m√©todos (comparar SHAP com LIME)
    C. Estabilidade das Explica√ß√µes
        - Vari√¢ncia da import√¢ncia de um mesmo atributo em diferentes inst√¢ncias
        - Aplique ANOVA ou Kruskal-Wallis para ver se h√° diferen√ßa significativa entre a distribui√ß√£o das import√¢ncias entre modelos
        - Clusteriza√ß√£o das esplica√ß√µes locais com K-Means ou DBSCAN para identificar padr√µes
    D. Fidelidade das explica√ß√µes locais (somente LIME)
        - Fidelidade √© a acur√°cia do modelo linear local que o LIME usa para explicar uma inst√¢ncia.
        - Salve esse valor para cada inst√¢ncia e calcule:
            - M√©dia
            - Desvio padr√£o
            - Compara√ß√µes entre modelos
    E. Tempo de execu√ß√£o
        - Tempo m√©dio para gerar explica√ß√µes com SHAP e LIME por modelo
        - Da pra usar time.time() para medir
    F. Visualiza√ß√µes √∫teis
        - summary_plot do SHAP
        - force_plot ou decision_plot para casos individuais
        - Gr√°fico de barras com as top-k import√¢ncias m√©dias
        - Heatmap de correla√ß√£o entre m√©todos (spearman)
        - Boxplots comparando a import√¢ncia dos atributos entre modelos
        - PCA ou t-SNE nas explica√ß√µes locais (para clusteriza√ß√£o visual)
    
üìå Exemplos de an√°lises estat√≠sticas
    - "Atributo X tem import√¢ncia significativamente maior no RF comparado ao MLP?"
    - Teste t de Student (se normalidade ok) ou Mann-Whitney U
    - "A ordem dos atributos mais importantes √© semelhante entre SHAP e LIME?"
    - Correla√ß√£o de Spearman
    - "A fidelidade do LIME √© maior no modelo RF comparado ao MLP?"
    - ANOVA ou teste de Kruskal-Wallis

‚úÖ Conclus√£o da an√°lise (no seu TCC ou artigo) - Voc√™ poder√° responder:
    - Quais modelos s√£o mais transparentes (t√™m explica√ß√µes mais est√°veis, coerentes, r√°pidas)?
    - SHAP ou LIME fornece explica√ß√µes mais fi√©is no seu contexto?
    - H√° atributos-chave que aparecem consistentemente em todos os modelos?
    - Qual m√©todo XAI mostra maior sensibilidade a mudan√ßas no modelo?
    

https://www.datacamp.com/tutorial/introduction-to-shap-values-machine-learning-interpretability
